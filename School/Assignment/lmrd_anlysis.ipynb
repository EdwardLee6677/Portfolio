{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Local\\Temp\\ipykernel_26116\\3238638070.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  all_data = read_csv('lmrd_review.csv', delimiter=',', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "all_data = read_csv('lmrd_review.csv', delimiter=',', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
       "1  Homelessness (or Houselessness as George Carli...      1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
       "3  This is easily the most underrated film inn th...      1\n",
       "4  This is not the typical Mel Brooks film. It wa...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['review'] = all_data['review'].str.replace('\\n', ' ')\n",
    "all_data['review'] = all_data['review'].str.replace('\\r', ' ')\n",
    "all_data['review'] = all_data['review'].str.replace('\\t', ' ')\n",
    "all_data['review'] = all_data['review'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dropna(subset=['review'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edward\\anaconda3\\envs\\py\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Logistic Regression ]\n",
      "정확도: 0.87\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      4945\n",
      "           1       0.87      0.87      0.87      5055\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "[ Naive Bayes ]\n",
      "정확도: 0.85\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      4945\n",
      "           1       0.85      0.84      0.85      5055\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "[ Random Forest ]\n",
      "정확도: 0.85\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85      4945\n",
      "           1       0.86      0.83      0.84      5055\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "[ XGBoost ]\n",
      "정확도: 0.86\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      4945\n",
      "           1       0.85      0.87      0.86      5055\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "[ LightGBM ]\n",
      "정확도: 0.86\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      4945\n",
      "           1       0.85      0.87      0.86      5055\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "[ SGD ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edward\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.88\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      4945\n",
      "           1       0.89      0.86      0.88      5055\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "모델 별 정확도 비교:\n",
      "Logistic Regression: 0.87\n",
      "Naive Bayes: 0.85\n",
      "Random Forest: 0.85\n",
      "XGBoost: 0.86\n",
      "LightGBM: 0.86\n",
      "SGD: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 데이터 분리\n",
    "X = all_data[\"review\"]\n",
    "y = all_data[\"label\"]\n",
    "\n",
    "# 데이터셋을 학습용(80%)과 테스트용(20%)으로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텍스트 벡터화\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=5000)  # 상위 5000개의 단어만 사용\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"SGD\": SGDClassifier(loss=\"log\", random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n[ {model_name} ]\")\n",
    "    \n",
    "    # XGBoost 모델의 경우 입력 데이터를 np.float32로 변환\n",
    "    if model_name == \"XGBoost\":\n",
    "        X_train_vec = X_train_vec.astype(np.float32)\n",
    "        X_test_vec = X_test_vec.astype(np.float32)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # 테스트 데이터로 예측\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[model_name] = accuracy\n",
    "    \n",
    "    # 평가 결과 출력\n",
    "    print(f\"정확도: {accuracy:.2f}\")\n",
    "    print(\"분류 리포트:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 결과 비교\n",
    "print(\"\\n모델 별 정확도 비교:\")\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Logistic Regression ]\n",
      "\n",
      "부정 단어 상위 20개:\n",
      "forgettable: -2.2855\n",
      "waste: -2.2327\n",
      "tripe: -2.2044\n",
      "mst3k: -2.2024\n",
      "uninspired: -2.1701\n",
      "stinker: -2.0798\n",
      "disappointment: -2.0553\n",
      "poorly: -1.9404\n",
      "uninteresting: -1.8908\n",
      "wasting: -1.8608\n",
      "obnoxious: -1.8185\n",
      "mildly: -1.7950\n",
      "worst: -1.7465\n",
      "yawn: -1.7411\n",
      "rourke: -1.7348\n",
      "laughable: -1.7317\n",
      "tedious: -1.6931\n",
      "manipulative: -1.6416\n",
      "dreadful: -1.6185\n",
      "appalling: -1.5889\n",
      "\n",
      "긍정 단어 상위 20개:\n",
      "pitch: 1.2928\n",
      "suitable: 1.2988\n",
      "disappoint: 1.2989\n",
      "complaint: 1.3027\n",
      "complain: 1.3192\n",
      "funniest: 1.3344\n",
      "chilling: 1.3562\n",
      "delightful: 1.3615\n",
      "likewise: 1.3760\n",
      "superbly: 1.3846\n",
      "damned: 1.4089\n",
      "superb: 1.4312\n",
      "astonishing: 1.4416\n",
      "underrated: 1.5009\n",
      "fears: 1.5075\n",
      "polished: 1.5510\n",
      "segments: 1.5718\n",
      "notch: 1.5753\n",
      "finest: 1.6016\n",
      "refreshing: 1.9081\n",
      "\n",
      "[ Naive Bayes ]\n",
      "Naive Bayes 모델은 가중치 또는 중요도를 제공하지 않음.\n",
      "\n",
      "[ Random Forest ]\n",
      "\n",
      "부정 단어 상위 20개:\n",
      "ponyo: 0.0000\n",
      "custer: 0.0000\n",
      "hartley: 0.0000\n",
      "venice: 0.0000\n",
      "miyazaki: 0.0000\n",
      "chess: 0.0000\n",
      "abusive: 0.0000\n",
      "widmark: 0.0000\n",
      "cannibal: 0.0000\n",
      "macbeth: 0.0000\n",
      "witches: 0.0000\n",
      "niro: 0.0000\n",
      "che: 0.0000\n",
      "hippie: 0.0000\n",
      "cagney: 0.0000\n",
      "boll: 0.0000\n",
      "mann: 0.0000\n",
      "warriors: 0.0000\n",
      "lincoln: 0.0000\n",
      "newman: 0.0000\n",
      "\n",
      "긍정 단어 상위 20개:\n",
      "amazing: 0.0038\n",
      "movie: 0.0040\n",
      "perfect: 0.0041\n",
      "plot: 0.0046\n",
      "minutes: 0.0048\n",
      "horrible: 0.0052\n",
      "wonderful: 0.0053\n",
      "love: 0.0054\n",
      "poor: 0.0054\n",
      "stupid: 0.0055\n",
      "worse: 0.0056\n",
      "best: 0.0069\n",
      "boring: 0.0072\n",
      "terrible: 0.0080\n",
      "excellent: 0.0092\n",
      "waste: 0.0105\n",
      "awful: 0.0114\n",
      "great: 0.0139\n",
      "worst: 0.0167\n",
      "bad: 0.0216\n",
      "\n",
      "[ XGBoost ]\n",
      "\n",
      "부정 단어 상위 20개:\n",
      "00: 0.0000\n",
      "offended: 0.0000\n",
      "offered: 0.0000\n",
      "offering: 0.0000\n",
      "offers: 0.0000\n",
      "office: 0.0000\n",
      "officer: 0.0000\n",
      "officers: 0.0000\n",
      "official: 0.0000\n",
      "oil: 0.0000\n",
      "older: 0.0000\n",
      "oliver: 0.0000\n",
      "olivier: 0.0000\n",
      "online: 0.0000\n",
      "open: 0.0000\n",
      "odyssey: 0.0000\n",
      "opened: 0.0000\n",
      "operation: 0.0000\n",
      "opinions: 0.0000\n",
      "opposite: 0.0000\n",
      "\n",
      "긍정 단어 상위 20개:\n",
      "avoid: 0.0054\n",
      "dull: 0.0057\n",
      "stupid: 0.0058\n",
      "great: 0.0063\n",
      "horrible: 0.0064\n",
      "brilliant: 0.0066\n",
      "lame: 0.0070\n",
      "amazing: 0.0070\n",
      "wonderful: 0.0082\n",
      "poorly: 0.0083\n",
      "terrible: 0.0086\n",
      "poor: 0.0089\n",
      "boring: 0.0092\n",
      "crap: 0.0098\n",
      "worse: 0.0099\n",
      "excellent: 0.0107\n",
      "bad: 0.0142\n",
      "awful: 0.0147\n",
      "waste: 0.0173\n",
      "worst: 0.0230\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CountVectorizer에서 단어 목록 가져오기\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 모델 별 긍정/부정 단어 상위 20개 추출 함수\n",
    "def extract_top_words(model, feature_names, model_name):\n",
    "    if hasattr(model, \"coef_\"):  # 선형 모델 (Logistic Regression, Linear SVM)\n",
    "        coefficients = model.coef_[0]\n",
    "        sorted_indices = np.argsort(coefficients)\n",
    "        negative_top20 = [(feature_names[i], coefficients[i]) for i in sorted_indices[:20]]\n",
    "        positive_top20 = [(feature_names[i], coefficients[i]) for i in sorted_indices[-20:]]\n",
    "    elif hasattr(model, \"feature_importances_\"):  # 트리 기반 모델 (Random Forest)\n",
    "        importances = model.feature_importances_\n",
    "        sorted_indices = np.argsort(importances)\n",
    "        negative_top20 = [(feature_names[i], importances[i]) for i in sorted_indices[:20]]\n",
    "        positive_top20 = [(feature_names[i], importances[i]) for i in sorted_indices[-20:]]\n",
    "    else:\n",
    "        print(f\"{model_name} 모델은 가중치 또는 중요도를 제공하지 않음.\")\n",
    "        return None, None\n",
    "\n",
    "    return negative_top20, positive_top20\n",
    "\n",
    "# 각 모델에 대해 단어 추출\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n[ {model_name} ]\")\n",
    "    negative_top20, positive_top20 = extract_top_words(model, feature_names, model_name)\n",
    "    \n",
    "    if negative_top20 and positive_top20:\n",
    "        print(\"\\n부정 단어 상위 20개:\")\n",
    "        for word, weight in negative_top20:\n",
    "            print(f\"{word}: {weight:.4f}\")\n",
    "        \n",
    "        print(\"\\n긍정 단어 상위 20개:\")\n",
    "        for word, weight in positive_top20:\n",
    "            print(f\"{word}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\Edward\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk import download\n",
    "\n",
    "# 감성 사전 다운로드\n",
    "download(\"opinion_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 긍정/부정 단어 리스트 로드\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "# 텍스트 전처리 및 감성 단어 카운트 함수\n",
    "def preprocess_and_count_sentiment(text):\n",
    "    # 텍스트 소문자로 변환\n",
    "    text = text.lower()\n",
    "    # 특수 문자 및 숫자 제거\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "\n",
    "    # 긍정/부정 단어 카운트\n",
    "    pos_count = sum(1 for word in words if word in positive_words)\n",
    "    neg_count = sum(1 for word in words if word in negative_words)\n",
    "    return text, pos_count, neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant overacting by lesley ann warren best...</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  \\\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1   \n",
       "1  Homelessness (or Houselessness as George Carli...      1   \n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
       "3  This is easily the most underrated film inn th...      1   \n",
       "4  This is not the typical Mel Brooks film. It wa...      1   \n",
       "\n",
       "                                      review_cleaned  pos_count  neg_count  \n",
       "0  bromwell high is a cartoon comedy it ran at th...          5          5  \n",
       "1  homelessness or houselessness as george carlin...         17         14  \n",
       "2  brilliant overacting by lesley ann warren best...          9          7  \n",
       "3  this is easily the most underrated film inn th...          8          6  \n",
       "4  this is not the typical mel brooks film it was...          6          2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 및 감성 단어 카운트 적용\n",
    "all_data[\"review_cleaned\"], all_data[\"pos_count\"], all_data[\"neg_count\"] = zip(\n",
    "    *all_data[\"review\"].apply(preprocess_and_count_sentiment)\n",
    ")\n",
    "\n",
    "# 데이터 확인\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Logistic Regression ]\n",
      "정확도: 0.87\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      4945\n",
      "           1       0.87      0.87      0.87      5055\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "[ Naive Bayes ]\n",
      "정확도: 0.86\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85      4945\n",
      "           1       0.86      0.85      0.86      5055\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "[ Random Forest ]\n",
      "정확도: 0.85\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      4945\n",
      "           1       0.85      0.84      0.85      5055\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "[ XGBoost ]\n",
      "정확도: 0.86\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      4945\n",
      "           1       0.85      0.87      0.86      5055\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "[ LightGBM ]\n",
      "정확도: 0.86\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      4945\n",
      "           1       0.86      0.87      0.86      5055\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "[ SGD ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edward\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.87\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.87      4945\n",
      "           1       0.85      0.91      0.88      5055\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "모델 별 정확도 비교:\n",
      "Logistic Regression: 0.87\n",
      "Naive Bayes: 0.86\n",
      "Random Forest: 0.85\n",
      "XGBoost: 0.86\n",
      "LightGBM: 0.86\n",
      "SGD: 0.87\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# 기존 텍스트 벡터화\n",
    "X_text_vec = vectorizer.fit_transform(all_data[\"review_cleaned\"])\n",
    "\n",
    "# 긍정/부정 단어 카운트 추가\n",
    "X_features = hstack([X_text_vec, all_data[[\"pos_count\", \"neg_count\"]].values])\n",
    "\n",
    "# 학습/테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, all_data[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n[ {model_name} ]\")\n",
    "    \n",
    "    # XGBoost 모델의 경우 입력 데이터를 np.float32로 변환\n",
    "    if model_name == \"XGBoost\":\n",
    "        X_train = X_train.astype(np.float32)\n",
    "        X_test = X_test.astype(np.float32)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 테스트 데이터로 예측\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[model_name] = accuracy\n",
    "    \n",
    "    # 평가 결과 출력\n",
    "    print(f\"정확도: {accuracy:.2f}\")\n",
    "    print(\"분류 리포트:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 결과 비교\n",
    "print(\"\\n모델 별 정확도 비교:\")\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도 기반 모델 별 정확도 비교:\n",
    "Logistic Regression: 0.87\n",
    "Naive Bayes: 0.85\n",
    "Random Forest: 0.85\n",
    "XGBoost: 0.86\n",
    "LightGBM: 0.86\n",
    "SGD: 0.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
